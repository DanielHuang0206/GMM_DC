{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 位移 = 0 (六位精度) ===\n",
      "- 行 1: DeltaDist = 0\n",
      "- 行 11: DeltaDist = 0\n",
      "- 行 14: DeltaDist = 0\n",
      "- 行 15: DeltaDist = 0\n",
      "- 行 21: DeltaDist = 0\n",
      "- 行 24: DeltaDist = 0\n",
      "- 行 30: DeltaDist = 0\n",
      "- 行 31: DeltaDist = 0\n",
      "- 行 32: DeltaDist = 0\n",
      "- 行 33: DeltaDist = 0\n",
      "- 行 41: DeltaDist = 0\n",
      "- 行 44: DeltaDist = 0\n",
      "- 行 47: DeltaDist = 0\n",
      "- 行 50: DeltaDist = 0\n",
      "- 行 51: DeltaDist = 0\n",
      "- 行 57: DeltaDist = 0\n",
      "- 行 58: DeltaDist = 0\n",
      "- 行 64: DeltaDist = 0\n",
      "- 行 69: DeltaDist = 0\n",
      "- 行 70: DeltaDist = 0\n",
      "- 行 71: DeltaDist = 0\n",
      "- 行 72: DeltaDist = 0\n",
      "- 行 73: DeltaDist = 0\n",
      "- 行 74: DeltaDist = 0\n",
      "- 行 77: DeltaDist = 0\n",
      "- 行 96: DeltaDist = 0\n",
      "- 行 99: DeltaDist = 0\n",
      "- 行 102: DeltaDist = 0\n",
      "- 行 105: DeltaDist = 0\n",
      "- 行 109: DeltaDist = 0\n",
      "- 行 110: DeltaDist = 0\n",
      "- 行 113: DeltaDist = 0\n",
      "- 行 114: DeltaDist = 0\n",
      "- 行 118: DeltaDist = 0\n",
      "- 行 119: DeltaDist = 0\n",
      "================================\n",
      "\n",
      "✅ 完成！輸出：/media/dc0206/Crucial X6/GMM20/20240329_DATA/NTHU_5x/red06/Result_with_disp.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "固定讀取指定 Result.txt，計算相鄰兩筆位移量 (6 位小數)，並輸出 CSV。\n",
    "執行：\n",
    "    python compute_displacement_fixed.py\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import math\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== 1. 指定檔案路徑 ==========================================\n",
    "TXT_FILE = Path(\"/media/dc0206/Crucial X6/GMM20/20240329_DATA/NTHU_5x/red06/Result.txt\")\n",
    "# ==============================================================\n",
    "\n",
    "# 欲採用的欄位\n",
    "X_COL = \"AeroCmdX\"\n",
    "Y_COL = \"AeroCmdY\"\n",
    "\n",
    "PAIR_RE = re.compile(r\"([\\w\\d]+):\\s*([-+]?[0-9]*\\.?[0-9]+)\")\n",
    "\n",
    "def parse_line(line: str):\n",
    "    return {k: float(v) for k, v in PAIR_RE.findall(line)}\n",
    "\n",
    "def main():\n",
    "    if not TXT_FILE.exists():\n",
    "        raise FileNotFoundError(f\"找不到檔案：{TXT_FILE}\")\n",
    "\n",
    "    rows = []\n",
    "    with TXT_FILE.open(encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rows.append(parse_line(line))\n",
    "    if not rows:\n",
    "        raise ValueError(\"檔案內容無可解析資料\")\n",
    "\n",
    "    # --- 計算 ΔX, ΔY, ΔDist ----------------------------------\n",
    "    prev_x = prev_y = None\n",
    "    for r in rows:\n",
    "        cur_x, cur_y = r.get(X_COL), r.get(Y_COL)\n",
    "        if prev_x is None:\n",
    "            dx = dy = dist = 0.0\n",
    "        else:\n",
    "            dx, dy = cur_x - prev_x, cur_y - prev_y\n",
    "            dist = math.hypot(dx, dy)\n",
    "        # 🔸 四捨五入至 6 位\n",
    "        r.update(\n",
    "            DeltaX=round(dx, 6),\n",
    "            DeltaY=round(dy, 6),\n",
    "            DeltaDist=round(dist, 6)\n",
    "        )\n",
    "        prev_x, prev_y = cur_x, cur_y\n",
    "\n",
    "    # --- 找出位移為 0 的紀錄 ---------------------------------\n",
    "    zero_moves = []\n",
    "    for idx, r in enumerate(rows, start=1):\n",
    "        if math.isclose(r[\"DeltaDist\"], 0.0, abs_tol=1e-10):   # 🔸\n",
    "            zero_moves.append((idx, r))\n",
    "\n",
    "    if zero_moves:\n",
    "        print(\"\\n=== 位移 = 0 (六位精度) ===\")\n",
    "        for idx, r in zero_moves:\n",
    "            if \"ImgName\" in r:\n",
    "                print(f\"- 行 {idx}: {r['ImgName']}\")\n",
    "            else:\n",
    "                print(f\"- 行 {idx}: DeltaDist = 0\")\n",
    "        print(\"================================\\n\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  沒有 DeltaDist = 0 的紀錄\\n\")\n",
    "\n",
    "    # --- 輸出 CSV (固定 6 位小數) -----------------------------\n",
    "    out_path = TXT_FILE.with_name(TXT_FILE.stem + \"_with_disp.csv\")\n",
    "    fieldnames = rows[0].keys()\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in rows:\n",
    "            # 🔸 轉成格式化字串，確保 6 位\n",
    "            r_fmt = {k: (f\"{v:.6f}\" if isinstance(v, float) else v) for k, v in r.items()}\n",
    "            writer.writerow(r_fmt)\n",
    "\n",
    "    print(f\"✅ 完成！輸出：{out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"多資料集點雲‑影像對齊批次處理程式\n",
    "------------------------------------------------\n",
    "使用方式：\n",
    "1. 先在 DATASETS 清單中填入每個資料集的路徑與檔名設定。\n",
    "2. 執行 python multi_dataset_registration.py\n",
    "   -> 程式會依序處理所有資料夾，結果各自輸出為 Excel。\n",
    "   -> 若某資料集失敗，錯誤訊息會列出，但不影響其他批次。\n",
    "\n",
    "程式大綱：\n",
    "    ├─ 演算法函式（補點、DBSCAN 過濾、USAC、ICP/CPD...）\n",
    "    ├─ process_dataset(cfg): 處理單一資料集\n",
    "    └─ main(): 迴圈呼叫 process_dataset()\n",
    "\"\"\"\n",
    "\n",
    "# === 0. 套件 ===\n",
    "import os, glob, time\n",
    "import numpy as np, pandas as pd, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pycpd import RigidRegistration  # pip install pycpd\n",
    "\n",
    "# ---------------------------\n",
    "# 1. 補點函式\n",
    "# ---------------------------\n",
    "\n",
    "def ensure_two_points_in_circle_all(filtered_points, radius: float = 10.0):\n",
    "    \"\"\"確保每個圓形區域內至少兩點，如不足則補點\"\"\"\n",
    "    if len(filtered_points) < 1:\n",
    "        return filtered_points, []\n",
    "\n",
    "    new_points = []\n",
    "    for i, center in enumerate(filtered_points):\n",
    "        dists = np.linalg.norm(filtered_points - center, axis=1)\n",
    "        in_circle_indices = np.where(dists <= radius)[0]\n",
    "        if len(in_circle_indices) < 2:\n",
    "            if len(filtered_points) == 1:\n",
    "                rand_dir = np.random.randn(2)\n",
    "                rand_dir /= (np.linalg.norm(rand_dir) + 1e-9)\n",
    "                new_pt = center + rand_dir * (radius * 0.5)\n",
    "                new_points.append(new_pt)\n",
    "            else:\n",
    "                sorted_idx = np.argsort(dists)\n",
    "                nearest_idx = sorted_idx[1] if sorted_idx[0] == i else sorted_idx[0]\n",
    "                nearest_point = filtered_points[nearest_idx]\n",
    "                direction_vec = center - nearest_point\n",
    "                dist_np = np.linalg.norm(direction_vec)\n",
    "                if dist_np < 1e-9:\n",
    "                    direction_vec = np.random.randn(2)\n",
    "                    dist_np = np.linalg.norm(direction_vec)\n",
    "                dir_unit = direction_vec / dist_np\n",
    "                new_pt = center + dir_unit * (radius * 0.8)\n",
    "                new_points.append(new_pt)\n",
    "\n",
    "    if len(new_points) > 0:\n",
    "        new_points = np.array(new_points)\n",
    "        updated_points = np.vstack([filtered_points, new_points])\n",
    "    else:\n",
    "        updated_points = filtered_points\n",
    "    return updated_points, new_points\n",
    "\n",
    "def ensure_two_points_in_circle_iterative(points, radius: float = 10.0, max_iter: int = 100):\n",
    "    updated_points = points.copy()\n",
    "    for _ in range(max_iter):\n",
    "        updated_points, new_points = ensure_two_points_in_circle_all(updated_points, radius=radius)\n",
    "        if len(new_points) == 0:\n",
    "            break\n",
    "    return updated_points\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 雜訊過濾與輔助函式\n",
    "# ---------------------------\n",
    "\n",
    "def filter_noise_points_weighted(points, eps: float = 3, min_samples: int = 2,\n",
    "                                 min_cluster_size: int = 55, return_mask: bool = False):\n",
    "    if len(points) == 0:\n",
    "        return np.array([]), np.array([]) if not return_mask else (np.array([]), np.array([]), None)\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(points)\n",
    "    labels = db.labels_\n",
    "    unique_labels, counts = np.unique(labels[labels != -1], return_counts=True)\n",
    "    large_clusters = set(unique_labels[counts >= min_cluster_size])\n",
    "    mask = np.isin(labels, list(large_clusters))\n",
    "    filtered_points = points[mask]\n",
    "    return (filtered_points, mask) if not return_mask else (filtered_points, mask, mask)\n",
    "\n",
    "def compute_local_density(points, k: int = 18):\n",
    "    if len(points) == 0:\n",
    "        return np.array([])\n",
    "    actual_k = min(k + 1, len(points))\n",
    "    nbrs = NearestNeighbors(n_neighbors=actual_k).fit(points)\n",
    "    distances, _ = nbrs.kneighbors(points)\n",
    "    avg_distance = np.mean(distances[:, 1:], axis=1)\n",
    "    density = 1.0 / (avg_distance + 1e-6)\n",
    "    return density\n",
    "\n",
    "def estimate_rigid_transform(src, dst):\n",
    "    centroid_src = np.mean(src, axis=0)\n",
    "    centroid_dst = np.mean(dst, axis=0)\n",
    "    src_centered = src - centroid_src\n",
    "    dst_centered = dst - centroid_dst\n",
    "    H = src_centered.T @ dst_centered\n",
    "    U, _, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[1, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "    t = centroid_dst - R @ centroid_src\n",
    "    return R, t\n",
    "\n",
    "def usac_rigid_transform(src_points, dst_points, *, num_iterations: int = 100,\n",
    "                         inlier_threshold: float = 50.0, min_inliers: int = 60):\n",
    "    best_inlier_count = 0\n",
    "    best_R = best_t = best_inlier_mask = None\n",
    "    N = src_points.shape[0]\n",
    "    if N < 2:\n",
    "        raise ValueError(\"來源點數不足\")\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        if N < 8:\n",
    "            break\n",
    "        indices = np.random.choice(N, 8, replace=False)\n",
    "        src_sample = src_points[indices]\n",
    "        dst_sample = dst_points[indices]\n",
    "        if np.linalg.norm(src_sample[1] - src_sample[0]) < 1e-10:\n",
    "            continue\n",
    "        R_cand, t_cand = estimate_rigid_transform(src_sample, dst_sample)\n",
    "        src_transformed = (R_cand @ src_points.T).T + t_cand\n",
    "        distances = np.linalg.norm(dst_points - src_transformed, axis=1)\n",
    "        inlier_mask = distances < inlier_threshold\n",
    "        inlier_count = np.sum(inlier_mask)\n",
    "        if inlier_count > best_inlier_count and inlier_count >= min_inliers:\n",
    "            best_inlier_count = inlier_count\n",
    "            best_R, best_t, best_inlier_mask = R_cand, t_cand, inlier_mask\n",
    "            break\n",
    "    if best_inlier_mask is None or np.sum(best_inlier_mask) < min_inliers:\n",
    "        raise ValueError(\"USAC 未找到足夠內點\")\n",
    "    src_inliers = src_points[best_inlier_mask]\n",
    "    dst_inliers = dst_points[best_inlier_mask]\n",
    "    best_R, best_t = estimate_rigid_transform(src_inliers, dst_inliers)\n",
    "    return best_R, best_t, best_inlier_mask\n",
    "\n",
    "# ---------------------------\n",
    "# 3. ICP / CPD / Normal‑ICP\n",
    "# ---------------------------\n",
    "\n",
    "def icp_with_weights(source_points, target_points, weights, *, max_iterations: int = 500, tolerance: float = 1e-5):#-20\n",
    "    R_total = np.eye(2)\n",
    "    t_total = np.zeros(2)\n",
    "    prev_error = float(\"inf\")\n",
    "    for iteration in range(max_iterations):\n",
    "        transformed_source = (R_total @ source_points.T).T + t_total\n",
    "        nbrs = NearestNeighbors(n_neighbors=1).fit(target_points)\n",
    "        distances, indices = nbrs.kneighbors(transformed_source)\n",
    "        closest_points = target_points[indices.flatten()]\n",
    "        error = np.sum(weights * (distances.flatten() ** 2)) / (np.sum(weights) + 1e-9)\n",
    "        if abs(prev_error - error) < tolerance and error < 1:\n",
    "            break\n",
    "        prev_error = error\n",
    "        src_centroid = np.average(transformed_source, axis=0, weights=weights)\n",
    "        tgt_centroid = np.average(closest_points, axis=0, weights=weights)\n",
    "        src_cent = transformed_source - src_centroid\n",
    "        tgt_cent = closest_points - tgt_centroid\n",
    "        H = (weights[:, None] * src_cent).T @ tgt_cent\n",
    "        U, _, Vt = np.linalg.svd(H)\n",
    "        R_delta = Vt.T @ U.T\n",
    "        if np.linalg.det(R_delta) < 0:\n",
    "            Vt[1, :] *= -1\n",
    "            R_delta = Vt.T @ U.T\n",
    "        t_delta = tgt_centroid - R_delta @ src_centroid\n",
    "        R_total = R_delta @ R_total\n",
    "        t_total = R_delta @ t_total + t_delta\n",
    "    return R_total, t_total, prev_error\n",
    "\n",
    "def cpd_rigid_registration(source_points, target_points, *, max_iterations: int = 50, tolerance: float = 1e-3):\n",
    "    reg = RigidRegistration(X=target_points, Y=source_points,\n",
    "                            max_iterations=max_iterations, tolerance=tolerance)\n",
    "    Y_reg, (s, R, t) = reg.register()\n",
    "    return s, R, t, Y_reg\n",
    "\n",
    "def compute_normals(points, k: int = 40):\n",
    "    if len(points) < 3:\n",
    "        return np.zeros_like(points)\n",
    "    actual_k = min(k, len(points) - 1)\n",
    "    nbrs = NearestNeighbors(n_neighbors=actual_k).fit(points)\n",
    "    _, indices = nbrs.kneighbors(points)\n",
    "    normals = np.zeros_like(points)\n",
    "    for i, neighbors in enumerate(indices):\n",
    "        cov_matrix = np.cov(points[neighbors].T)\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov_matrix)\n",
    "        normals[i] = eigvecs[:, 0]\n",
    "    return normals / (np.linalg.norm(normals, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "def normal_icp(source_points, target_points, weights, *, alpha: float = 0.1, gamma: float = 0.8,\n",
    "               max_iterations: int = 500, tolerance: float = 1e-5):\n",
    "    source_normals = compute_normals(source_points)\n",
    "    target_normals = compute_normals(target_points)\n",
    "    R_total = np.eye(2)\n",
    "    t_total = np.zeros(2)\n",
    "    prev_error = float(\"inf\")\n",
    "    for iteration in range(max_iterations):\n",
    "        transformed_source = (R_total @ source_points.T).T + t_total\n",
    "        nbrs = NearestNeighbors(n_neighbors=1).fit(target_points)\n",
    "        distances, indices = nbrs.kneighbors(transformed_source)\n",
    "        indices = np.clip(indices.flatten(), 0, len(target_normals) - 1)\n",
    "        closest_points = target_points[indices]\n",
    "        closest_normals = target_normals[indices]\n",
    "        normal_dot = np.einsum(\"ij,ij->i\", source_normals, closest_normals)\n",
    "        normal_errors = np.arccos(np.clip(normal_dot, -1, 1))\n",
    "        weighted_normals = np.exp(-normal_errors / (2 * alpha)) ** gamma\n",
    "        combined_weights = weights * distances.flatten() * weighted_normals\n",
    "        error = np.sum(combined_weights * (distances.flatten() ** 2)) / (np.sum(combined_weights) + 1e-9)\n",
    "        if abs(prev_error - error) < tolerance and error < 1:\n",
    "            break\n",
    "        prev_error = error\n",
    "        src_centroid = np.average(transformed_source, axis=0, weights=combined_weights)\n",
    "        tgt_centroid = np.average(closest_points, axis=0, weights=combined_weights)\n",
    "        src_cent = transformed_source - src_centroid\n",
    "        tgt_cent = closest_points - tgt_centroid\n",
    "        H = (combined_weights[:, None] * src_cent).T @ tgt_cent\n",
    "        U, _, Vt = np.linalg.svd(H)\n",
    "        R_delta = Vt.T @ U.T\n",
    "        if np.linalg.det(R_delta) < 0:\n",
    "            Vt[1, :] *= -1\n",
    "            R_delta = Vt.T @ U.T\n",
    "        t_delta = tgt_centroid - R_delta @ src_centroid\n",
    "        R_total = R_delta @ R_total\n",
    "        t_total = R_delta @ t_total + t_delta\n",
    "    return R_total, t_total, prev_error\n",
    "\n",
    "def fill_line_gaps(points, *, max_distance: float = 3.0, max_iter: int = 5):\n",
    "    pts = points.copy()\n",
    "    for _ in range(max_iter):\n",
    "        nbrs = NearestNeighbors(n_neighbors=2).fit(pts)\n",
    "        distances, indices = nbrs.kneighbors(pts)\n",
    "        new_pts = []\n",
    "        for i in range(len(pts)):\n",
    "            d = distances[i, 1]\n",
    "            if d > max_distance:\n",
    "                p1, p2 = pts[i], pts[indices[i, 1]]\n",
    "                num_insert = int(d // max_distance)\n",
    "                for n in range(1, num_insert + 1):\n",
    "                    new_pts.append(p1 + (p2 - p1) * n / (num_insert + 1))\n",
    "        if not new_pts:\n",
    "            break\n",
    "        pts = np.vstack([pts, np.array(new_pts)])\n",
    "    return pts\n",
    "\n",
    "\"\"\"{\n",
    "        \"name\": \"red01\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red01\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red01\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red01/Image_20240321100710525.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321100710525.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red01_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red02\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red02\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red02\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red02/Image_20240321104026349.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321104026349.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red02_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red03\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red03\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red03\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red03/Image_20240321105545542.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321105545542.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red03_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red04\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red04\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red04\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red04/Image_20240321111055812.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321111055812.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red04_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red05\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red05\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red05\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red05/Image_20240321112738655.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321112738655.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red05_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red06\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red06\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red06\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red06/Image_20240321114408009.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321114408009.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red06_combined.xlsx\",\n",
    "    },\n",
    "\"\"\"\n",
    "# ---------------------------\n",
    "# 4. DATASETS 參數清單\n",
    "# ---------------------------\n",
    "DATASETS: List[Dict] = [\n",
    "{\n",
    "        \"name\": \"red01\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red01\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red01\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red01/Image_20240321100710525.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321100710525.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red01_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red02\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red02\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red02\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red02/Image_20240321104026349.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321104026349.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red02_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red03\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red03\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red03\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red03/Image_20240321105545542.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321105545542.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red03_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red04\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red04\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red04\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red04/Image_20240321111055812.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321111055812.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red04_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red05\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red05\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red05\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red05/Image_20240321112738655.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321112738655.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red05_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"red06\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red06\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/red06\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/red06/Image_20240321114408009.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321114408009.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/red06_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"white01\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white01\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/white01\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white01/Image_20240321143412883.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321143412883.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/white01_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"white02\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white02\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/white02\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white02/Image_20240321145312162.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321145312162.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/white02_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"white03\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white03\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/white03\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white03/Image_20240321151040617.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321151040617.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/white03_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"white04\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white04\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/white04\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white04/Image_20240321152853920.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321152853920.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/white04_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"white05\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white05\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/white05\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white05/Image_20240321154611642.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321154611642.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/white05_combined.xlsx\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"white06\",\n",
    "        \"csv_dir\":  r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white06\",\n",
    "        \"img_dir\":  r\"/media/dc0206/Crucial X6/GMM20/20240329_DATA2/20240329_DATA/NTHU_5x/white06\",\n",
    "        \"template_csv\": r\"/media/dc0206/Crucial X6/GMM20/EdgeFinderResult/white06/Image_20240321160945507.csv\",\n",
    "        \"template_img\": r\"/media/dc0206/Crucial X6/GMM20/crop_w5.bmp\",\n",
    "        \"first_img\": \"Image_20240321160945507.bmp\",\n",
    "        \"final_xlsx\": r\"/media/dc0206/Crucial X6/GMM20/white06_combined.xlsx\",\n",
    "    },\n",
    "\n",
    "    # --- 新資料集請直接在此處繼續加入 ------------------------------------\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [red01] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red01_combined.xlsx，總耗時 42.5s\n",
      "✅ [red01] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red01_combined.xlsx，總耗時 42.6s\n",
      "✅ [red02] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red02_combined.xlsx，總耗時 46.3s\n",
      "✅ [red02] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red02_combined.xlsx，總耗時 46.4s\n",
      "✅ [red03] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red03_combined.xlsx，總耗時 27.4s\n",
      "✅ [red03] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red03_combined.xlsx，總耗時 27.5s\n",
      "✅ [red04] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red04_combined.xlsx，總耗時 29.4s\n",
      "✅ [red04] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red04_combined.xlsx，總耗時 29.4s\n",
      "✅ [red05] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red05_combined.xlsx，總耗時 28.1s\n",
      "✅ [red05] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red05_combined.xlsx，總耗時 28.2s\n",
      "✅ [red06] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red06_combined.xlsx，總耗時 28.7s\n",
      "✅ [red06] 完成，檔案：/media/dc0206/Crucial X6/GMM20/red06_combined.xlsx，總耗時 28.7s\n",
      "✅ [white01] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white01_combined.xlsx，總耗時 37.3s\n",
      "✅ [white01] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white01_combined.xlsx，總耗時 37.4s\n",
      "✅ [white02] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white02_combined.xlsx，總耗時 37.2s\n",
      "✅ [white02] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white02_combined.xlsx，總耗時 37.3s\n",
      "✅ [white03] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white03_combined.xlsx，總耗時 29.7s\n",
      "✅ [white03] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white03_combined.xlsx，總耗時 29.7s\n",
      "✅ [white04] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white04_combined.xlsx，總耗時 30.6s\n",
      "✅ [white04] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white04_combined.xlsx，總耗時 30.7s\n",
      "✅ [white05] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white05_combined.xlsx，總耗時 47.4s\n",
      "✅ [white05] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white05_combined.xlsx，總耗時 47.4s\n",
      "✅ [white06] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white06_combined.xlsx，總耗時 32.8s\n",
      "✅ [white06] 完成，檔案：/media/dc0206/Crucial X6/GMM20/white06_combined.xlsx，總耗時 32.9s\n"
     ]
    }
   ],
   "source": [
    "# 5. 單資料集完整流程\n",
    "\n",
    "def process_dataset(cfg: Dict):\n",
    "    start = time.time()\n",
    "    # === 讀 cfg ===\n",
    "    csv_directory     = cfg[\"csv_dir\"]\n",
    "    image_directory   = cfg[\"img_dir\"]\n",
    "    template_csv_path = cfg[\"template_csv\"]\n",
    "    template_path     = cfg[\"template_img\"]\n",
    "    first_img         = cfg[\"first_img\"]\n",
    "    final_output      = cfg[\"final_xlsx\"]\n",
    "\n",
    "    # === 步驟耗時統計 ===\n",
    "    step_times = {\n",
    "        \"template\": [],\n",
    "        \"csv_read\": [],\n",
    "        \"denoise\": [],\n",
    "        \"usac\": [],\n",
    "        \"icp\": [],\n",
    "    }\n",
    "\n",
    "# === 1. 第一張影像建立參考 ===\n",
    "# === 0. 讀取模板 ===\n",
    "    t0 = time.time()\n",
    "    template_data = pd.read_csv(template_csv_path)\n",
    "    template_points = template_data[['PosX','PosY']].values\n",
    "    first_image = cv2.imread(os.path.join(image_directory, first_img), cv2.IMREAD_GRAYSCALE)\n",
    "    template_img = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)\n",
    "    res_match = cv2.matchTemplate(first_image, template_img, cv2.TM_CCOEFF_NORMED)\n",
    "    _, _, _, first_tl = cv2.minMaxLoc(res_match)\n",
    "    first_br = ( first_tl[0] + template_img.shape[1],\n",
    "                first_tl[1] + template_img.shape[0] )\n",
    "    step_times[\"template\"].append(time.time() - t0)\n",
    "\n",
    "    # 篩選模板區域\n",
    "    template_pts_filtered = template_points[\n",
    "        (template_points[:,0] >= first_tl[0]) & (template_points[:,0] <= first_br[0]) &\n",
    "        (template_points[:,1] >= first_tl[1]) & (template_points[:,1] <= first_br[1])\n",
    "    ]\n",
    "\n",
    "    h,w = template_img.shape[:2]\n",
    "    # 用第一次的 first_tl 算正中間\n",
    "    region_center = np.array([ first_tl[0] + w/2.0,\n",
    "                            first_tl[1] + h/2.0 ])\n",
    "    if len(template_pts_filtered) < 3:\n",
    "        raise ValueError(\"模板區域點不足\")\n",
    "    \n",
    "    # === 1. 第一張影像建立參考 ===\n",
    "    t0 = time.time()\n",
    "    df_first = pd.read_csv(os.path.join(csv_directory, first_img.replace('.bmp','.csv')))\n",
    "    pts1 = df_first[['PosX','PosY']].values\n",
    "    cond1 = (\n",
    "        (pts1[:,0] >= first_tl[0]) & (pts1[:,0] <= first_br[0]) &\n",
    "        (pts1[:,1] >= first_tl[1]) & (pts1[:,1] <= first_br[1])\n",
    "    )\n",
    "    first_matched = pts1[cond1]\n",
    "    # 只取過濾後的點\n",
    "    ref = filter_noise_points_weighted(first_matched)[0]\n",
    "    ref = ensure_two_points_in_circle_iterative(ref, radius=0.5, max_iter=300)\n",
    "    ref = fill_line_gaps(ref, max_distance=2.0, max_iter=500)\n",
    "    step_times[\"csv_read\"].append(time.time() - t0)\n",
    "\n",
    "    # 初始化 results 與 prev_inc\n",
    "    results = []\n",
    "    prev_inc = np.zeros(2)\n",
    "\n",
    "    # 第一張直接存\n",
    "    results.append({\n",
    "        \"image_file\": first_img,\n",
    "        \"rotation_matrix\": np.eye(2).tolist(),\n",
    "        \"translation_vector\": [0,0],\n",
    "        \"incremental_displacement\": [0,0],\n",
    "        \"incremental_disp_norm\": 0.0,\n",
    "        \"displacement\": [0,0],\n",
    "        \"displacement_norm\": 0.0,\n",
    "        \"alignment_error\": 0.0,\n",
    "    })\n",
    "\n",
    "    # --- 迴圈處理其餘影像 ---\n",
    "    imgs = sorted(glob.glob(os.path.join(image_directory, '*.bmp')))\n",
    "    csvs = sorted(glob.glob(os.path.join(csv_directory, '*.csv')))\n",
    "    for idx in range(1, len(imgs)):\n",
    "        # 2-1 模板匹配找 tl, center\n",
    "        t0 = time.time()\n",
    "        im = cv2.imread(imgs[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        res = cv2.matchTemplate(im, template_img, cv2.TM_CCOEFF_NORMED)\n",
    "        _, _, _, tl = cv2.minMaxLoc(res)\n",
    "        step_times[\"template\"].append(time.time() - t0)\n",
    "        # 區域中心\n",
    "        region_center = np.array([ tl[0] + w/2.0,\n",
    "                                   tl[1] + h/2.0 ])\n",
    "\n",
    "        # 2-2 讀 CSV + 篩點\n",
    "        t0 = time.time()\n",
    "        dfc = pd.read_csv(csvs[idx])\n",
    "        pts = dfc[['PosX','PosY']].values\n",
    "        cond = ((pts[:,0]>=tl[0])&(pts[:,0]<=tl[0]+w)&\n",
    "                (pts[:,1]>=tl[1])&(pts[:,1]<=tl[1]+h))\n",
    "        cur = pts[cond]\n",
    "        step_times[\"csv_read\"].append(time.time() - t0)\n",
    "        if len(cur)<3 or len(ref)<3:\n",
    "            continue\n",
    "\n",
    "        # 2-3 雜訊過濾 + 補點\n",
    "        t0 = time.time()\n",
    "        # 只取過濾後的點\n",
    "        cur = filter_noise_points_weighted(cur)[0]\n",
    "        cur = ensure_two_points_in_circle_iterative(cur, radius=0.5, max_iter=300)\n",
    "        cur = fill_line_gaps(cur, max_distance=2.0, max_iter=500)\n",
    "        step_times[\"denoise\"].append(time.time() - t0)\n",
    "\n",
    "        # 2-4 權重\n",
    "        dens = compute_local_density(cur, k=25)\n",
    "        dens = (dens - dens.min())/(dens.max()-dens.min()+1e-9)\n",
    "\n",
    "        # 2-5 USAC\n",
    "        t0 = time.time()\n",
    "        nbrs = NearestNeighbors(n_neighbors=1).fit(ref)\n",
    "        _,idx2 = nbrs.kneighbors(cur)\n",
    "        tgt = ref[idx2.flatten()]\n",
    "        try:\n",
    "            R_u, t_u, _ = usac_rigid_transform(cur, tgt, inlier_threshold=40, min_inliers=20)\n",
    "        except:\n",
    "            R_u, t_u = np.eye(2), np.zeros(2)\n",
    "        step_times[\"usac\"].append(time.time() - t0)\n",
    "\n",
    "        # 2-6 ICP + CPD + Normal-ICP\n",
    "        t0 = time.time()\n",
    "        R_p, t_p, e_p = icp_with_weights(cur, ref, dens)\n",
    "        try:\n",
    "            _, R_c, t_c, _ = cpd_rigid_registration(cur, ref, max_iterations=100, tolerance=1e-3)\n",
    "        except:\n",
    "            R_c, t_c = np.eye(2), np.zeros(2)\n",
    "        R_n, t_n, e_n = normal_icp((R_c@cur.T).T + t_c, ref, dens)\n",
    "        if e_p < e_n:\n",
    "            R_f, t_f, err_f = R_p, t_p, e_p\n",
    "        else:\n",
    "            R_f = R_n @ R_c\n",
    "            t_f = R_n @ t_c + t_n\n",
    "            err_f = e_n\n",
    "        step_times[\"icp\"].append(time.time() - t0)\n",
    "        #print(region_center)\n",
    "        # 2-7 計算增量位移：考慮旋轉+平移的 region_center\n",
    "        transformed_center = R_f @ region_center + t_f\n",
    "        #transformed_center = region_center + t_f\n",
    "        inc = transformed_center - region_center\n",
    "        disp = inc - prev_inc\n",
    "        prev_inc = inc.copy()\n",
    "\n",
    "        # 2-8 收集結果\n",
    "        results.append({\n",
    "            \"image_file\": os.path.basename(imgs[idx]),\n",
    "            \"rotation_matrix\":          R_f.tolist(),\n",
    "            \"translation_vector\":       t_f.tolist(),\n",
    "            \"incremental_displacement\": inc.tolist(),\n",
    "            \"incremental_disp_norm\":    float(np.linalg.norm(inc)),\n",
    "            \"displacement\":             disp.tolist(),\n",
    "            \"displacement_norm\":        float(np.linalg.norm(disp)),\n",
    "            \"alignment_error\":          float(err_f),\n",
    "        })\n",
    "\n",
    "\n",
    "    # === 3. 輸出 Excel ===\n",
    "    os.makedirs(os.path.dirname(final_output), exist_ok=True)\n",
    "    pd.DataFrame(results).to_excel(final_output, index=False)\n",
    "    print(f\"✅ [{cfg['name']}] 完成，檔案：{final_output}，總耗時 {(time.time()-start):.1f}s\")\n",
    "    # === 4. 畫步驟耗時圓餅圖 ===\n",
    "    labels = list(step_times.keys())\n",
    "    totals = [sum(step_times[k]) for k in labels]\n",
    "    if sum(totals)>0:\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.pie(totals, labels=labels, autopct=\"%1.1f%%\", startangle=45)\n",
    "        plt.axis(\"equal\")\n",
    "        chart_dir = os.path.join(os.path.dirname(final_output),\"time_charts\")\n",
    "        os.makedirs(chart_dir, exist_ok=True)\n",
    "        pie_path = os.path.join(chart_dir, f\"{cfg['name']}_time_pie.png\")\n",
    "        plt.savefig(pie_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ [{cfg['name']}] 完成，檔案：{final_output}，總耗時 {(time.time()-start):.1f}s\")\n",
    "\n",
    "def main():\n",
    "    for cfg in DATASETS:\n",
    "        try:\n",
    "            process_dataset(cfg)\n",
    "        except Exception as e:\n",
    "            print(f\"[{cfg['name']}] 失敗：{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
